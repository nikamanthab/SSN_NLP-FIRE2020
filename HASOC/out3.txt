OFF    2019
NOT    2019
OFf       1
not       1
Name: Labels, dtype: int64
TFIDF:
precision: 0.8319576710182454
recall: 0.8225677711404682
f1: 0.8199333284957665
acc: 0.8225841883197533
Count Vec:
precision: 0.837095439074726
recall: 0.8250381365776329
f1: 0.8213539149749659
acc: 0.8250601713099114
MLP:
precision: 0.7476145843739266
recall: 0.738114739268958
f1: 0.7327006279099344
acc: 0.7381554254892306
Iteration 1, loss = 0.66161932
Iteration 2, loss = 0.60026480
Iteration 3, loss = 0.55119614
Iteration 4, loss = 0.51569159
Iteration 5, loss = 0.48979959
Iteration 6, loss = 0.46712648
Iteration 7, loss = 0.45130981
Iteration 8, loss = 0.44637256
Iteration 9, loss = 0.42586310
Iteration 10, loss = 0.40989615
Iteration 11, loss = 0.39971999
Iteration 12, loss = 0.39166026
Iteration 13, loss = 0.37860434
Iteration 14, loss = 0.36907041
Iteration 15, loss = 0.35790131
Iteration 16, loss = 0.34986261
Iteration 17, loss = 0.34640556
Iteration 18, loss = 0.34028834
Iteration 19, loss = 0.32563074
Iteration 20, loss = 0.31786250
Iteration 21, loss = 0.30918530
Iteration 22, loss = 0.30084043
Iteration 23, loss = 0.29674575
Iteration 24, loss = 0.28539717
Iteration 25, loss = 0.28450533
Iteration 26, loss = 0.27104688
Iteration 27, loss = 0.26381124
Iteration 28, loss = 0.26115411
Iteration 29, loss = 0.25178052
Iteration 30, loss = 0.24650397
Iteration 31, loss = 0.24020346
Iteration 32, loss = 0.23617299
Iteration 33, loss = 0.22801251
Iteration 34, loss = 0.22265774
Iteration 35, loss = 0.21615909
Iteration 36, loss = 0.21160827
Iteration 37, loss = 0.20982616
Iteration 38, loss = 0.20100188
Iteration 39, loss = 0.19682193
Iteration 40, loss = 0.19172811
Iteration 41, loss = 0.18415186
Iteration 42, loss = 0.18083762
Iteration 43, loss = 0.17461139
Iteration 44, loss = 0.17585733
Iteration 45, loss = 0.16509595
Iteration 46, loss = 0.16198548
Iteration 47, loss = 0.15818084
Iteration 48, loss = 0.15369451
Iteration 49, loss = 0.14924886
Iteration 50, loss = 0.15157266
Iteration 51, loss = 0.14319374
Iteration 52, loss = 0.13792284
Iteration 53, loss = 0.13781499
Iteration 54, loss = 0.13361218
Iteration 55, loss = 0.12752472
Iteration 56, loss = 0.12490438
Iteration 57, loss = 0.12214940
Iteration 58, loss = 0.11831615
Iteration 59, loss = 0.11692234
Iteration 60, loss = 0.11206556
Iteration 61, loss = 0.10853272
Iteration 62, loss = 0.10614386
Iteration 63, loss = 0.10626978
Iteration 64, loss = 0.10289470
Iteration 65, loss = 0.10360910
Iteration 66, loss = 0.09918837
Iteration 67, loss = 0.09549655
Iteration 68, loss = 0.09136473
Iteration 69, loss = 0.09169962
Iteration 70, loss = 0.09110108
Iteration 71, loss = 0.09004565
Iteration 72, loss = 0.08419010
Iteration 73, loss = 0.08155514
Iteration 74, loss = 0.08015346
Iteration 75, loss = 0.07900762
Iteration 76, loss = 0.07558105
Iteration 77, loss = 0.07366861
Iteration 78, loss = 0.07253071
Iteration 79, loss = 0.07285863
Iteration 80, loss = 0.07059969
Iteration 81, loss = 0.06765275
Iteration 82, loss = 0.06585995
Iteration 83, loss = 0.06561172
Iteration 84, loss = 0.06214900
Iteration 85, loss = 0.06234602
Iteration 86, loss = 0.06126337
Iteration 87, loss = 0.05898889
Iteration 88, loss = 0.05738169
Iteration 89, loss = 0.05551735
Iteration 90, loss = 0.05623091
Iteration 91, loss = 0.05716279
Iteration 92, loss = 0.05236332
Iteration 93, loss = 0.05426246
Iteration 94, loss = 0.05058880
Iteration 95, loss = 0.04947317
Iteration 96, loss = 0.04967711
Iteration 97, loss = 0.04864089
Iteration 98, loss = 0.04679053
Iteration 99, loss = 0.04693411
Iteration 100, loss = 0.04684933
Iteration 101, loss = 0.04450741
Iteration 102, loss = 0.04290052
Iteration 103, loss = 0.04378561
Iteration 104, loss = 0.04140027
Iteration 105, loss = 0.04225869
Iteration 106, loss = 0.04286361
Iteration 107, loss = 0.04105485
Iteration 108, loss = 0.03944244
Iteration 109, loss = 0.03972801
Iteration 110, loss = 0.03747342
Iteration 111, loss = 0.03694248
Iteration 112, loss = 0.03552194
Iteration 113, loss = 0.03681480
Iteration 114, loss = 0.03676431
Iteration 115, loss = 0.03560872
Iteration 116, loss = 0.03587360
Iteration 117, loss = 0.03307617
Iteration 118, loss = 0.03235650
Iteration 119, loss = 0.03186358
Iteration 120, loss = 0.03141399
Iteration 121, loss = 0.03236305
Iteration 122, loss = 0.03180474
Iteration 123, loss = 0.03217452
Iteration 124, loss = 0.02963670
Iteration 125, loss = 0.02932815
Iteration 126, loss = 0.03098853
Iteration 127, loss = 0.03187798
Iteration 128, loss = 0.02931042
Iteration 129, loss = 0.02971997
Iteration 130, loss = 0.02880073
Iteration 131, loss = 0.02774303
Iteration 132, loss = 0.02640549
Iteration 133, loss = 0.02599305
Iteration 134, loss = 0.02511101
Iteration 135, loss = 0.02531717
Iteration 136, loss = 0.02458630
Iteration 137, loss = 0.02401990
Iteration 138, loss = 0.02656165
Iteration 139, loss = 0.02478358
Iteration 140, loss = 0.02402763
Iteration 141, loss = 0.02315199
Iteration 142, loss = 0.02295549
Iteration 143, loss = 0.02300214
Iteration 144, loss = 0.02219077
Iteration 145, loss = 0.02300027
Iteration 146, loss = 0.02732876
Iteration 147, loss = 0.02559562
Iteration 148, loss = 0.02278932
Iteration 149, loss = 0.02350064
Iteration 150, loss = 0.02271313
Iteration 151, loss = 0.02169627
Iteration 152, loss = 0.02126992
Iteration 153, loss = 0.02058825
Iteration 154, loss = 0.02058053
Iteration 155, loss = 0.02060433
Iteration 156, loss = 0.02055067
Iteration 157, loss = 0.01978086
Iteration 158, loss = 0.02231402
Iteration 159, loss = 0.02155634
Iteration 160, loss = 0.02098025
Iteration 161, loss = 0.01965421
Iteration 162, loss = 0.01892411
Iteration 163, loss = 0.01913175
Iteration 164, loss = 0.01863156
Iteration 165, loss = 0.02042602
Iteration 166, loss = 0.01973106
Iteration 167, loss = 0.01906916
Iteration 168, loss = 0.01852570
Iteration 169, loss = 0.01791597
Iteration 170, loss = 0.01925871
Iteration 171, loss = 0.02155744
Iteration 172, loss = 0.02302595
Iteration 173, loss = 0.01944470
Iteration 174, loss = 0.01745008
Iteration 175, loss = 0.01716858
Iteration 176, loss = 0.01734974
Iteration 177, loss = 0.01645763
Iteration 178, loss = 0.01716260
Iteration 179, loss = 0.01714780
Iteration 180, loss = 0.02050031
Iteration 181, loss = 0.01903856
Iteration 182, loss = 0.01762836
Iteration 183, loss = 0.02074462
Iteration 184, loss = 0.01773092
Iteration 185, loss = 0.01821253
Iteration 186, loss = 0.01672572
Iteration 187, loss = 0.01644040
Iteration 188, loss = 0.02089152
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.66089965
Iteration 2, loss = 0.59892511
Iteration 3, loss = 0.55684892
Iteration 4, loss = 0.51810363
Iteration 5, loss = 0.49022722
Iteration 6, loss = 0.47155985
Iteration 7, loss = 0.45189475
Iteration 8, loss = 0.44249881
Iteration 9, loss = 0.42831408
Iteration 10, loss = 0.41330543
Iteration 11, loss = 0.39940793
Iteration 12, loss = 0.38749918
Iteration 13, loss = 0.37623951
Iteration 14, loss = 0.37045669
Iteration 15, loss = 0.35945106
Iteration 16, loss = 0.35191658
Iteration 17, loss = 0.34024409
Iteration 18, loss = 0.33521957
Iteration 19, loss = 0.32607039
Iteration 20, loss = 0.31747749
Iteration 21, loss = 0.31330303
Iteration 22, loss = 0.30443736
Iteration 23, loss = 0.29743768
Iteration 24, loss = 0.29139953
Iteration 25, loss = 0.28277879
Iteration 26, loss = 0.27381103
Iteration 27, loss = 0.26728397
Iteration 28, loss = 0.26074923
Iteration 29, loss = 0.25531566
Iteration 30, loss = 0.25085156
Iteration 31, loss = 0.24591274
Iteration 32, loss = 0.23838037
Iteration 33, loss = 0.23319967
Iteration 34, loss = 0.22601273
Iteration 35, loss = 0.22200113
Iteration 36, loss = 0.21786671
Iteration 37, loss = 0.21233103
Iteration 38, loss = 0.20635631
Iteration 39, loss = 0.20280184
Iteration 40, loss = 0.19824687
Iteration 41, loss = 0.19343334
Iteration 42, loss = 0.18969682
Iteration 43, loss = 0.18538080
Iteration 44, loss = 0.18091789
Iteration 45, loss = 0.17520280
Iteration 46, loss = 0.17527222
Iteration 47, loss = 0.16885414
Iteration 48, loss = 0.16560390
Iteration 49, loss = 0.16002185
Iteration 50, loss = 0.15784355
Iteration 51, loss = 0.15535041
Iteration 52, loss = 0.14766756
Iteration 53, loss = 0.14552625
Iteration 54, loss = 0.14317969
Iteration 55, loss = 0.13655411
Iteration 56, loss = 0.13419270
Iteration 57, loss = 0.12988543
Iteration 58, loss = 0.12882580
Iteration 59, loss = 0.12498115
Iteration 60, loss = 0.12039244
Iteration 61, loss = 0.12068156
Iteration 62, loss = 0.11452343
Iteration 63, loss = 0.11477481
Iteration 64, loss = 0.11164297
Iteration 65, loss = 0.10916847
Iteration 66, loss = 0.10440434
Iteration 67, loss = 0.10326614
Iteration 68, loss = 0.10131768
Iteration 69, loss = 0.09894626
Iteration 70, loss = 0.10178032
Iteration 71, loss = 0.09352003
Iteration 72, loss = 0.09369174
Iteration 73, loss = 0.09059364
Iteration 74, loss = 0.08650518
Iteration 75, loss = 0.08395703
Iteration 76, loss = 0.08221246
Iteration 77, loss = 0.08168570
Iteration 78, loss = 0.08181080
Iteration 79, loss = 0.07715082
Iteration 80, loss = 0.07661948
Iteration 81, loss = 0.07313366
Iteration 82, loss = 0.07259412
Iteration 83, loss = 0.06995149
Iteration 84, loss = 0.06817693
Iteration 85, loss = 0.06763303
Iteration 86, loss = 0.06589170
Iteration 87, loss = 0.06499349
Iteration 88, loss = 0.06339649
Iteration 89, loss = 0.06117278
Iteration 90, loss = 0.06202885
Iteration 91, loss = 0.05854460
Iteration 92, loss = 0.05741190
Iteration 93, loss = 0.05612524
Iteration 94, loss = 0.05504343
Iteration 95, loss = 0.05358494
Iteration 96, loss = 0.05261599
Iteration 97, loss = 0.05126743
Iteration 98, loss = 0.05103011
Iteration 99, loss = 0.05074056
Iteration 100, loss = 0.05050506
Iteration 101, loss = 0.04865485
Iteration 102, loss = 0.04881179
Iteration 103, loss = 0.04615824
Iteration 104, loss = 0.04616621
Iteration 105, loss = 0.05063871
Iteration 106, loss = 0.04450808
Iteration 107, loss = 0.04305079
Iteration 108, loss = 0.04108848
Iteration 109, loss = 0.04205192
Iteration 110, loss = 0.04017088
Iteration 111, loss = 0.03923631
Iteration 112, loss = 0.04288956
Iteration 113, loss = 0.04054223
Iteration 114, loss = 0.03833137
Iteration 115, loss = 0.03813115
Iteration 116, loss = 0.03652588
Iteration 117, loss = 0.03550525
Iteration 118, loss = 0.03507631
Iteration 119, loss = 0.03373457
Iteration 120, loss = 0.03368983
Iteration 121, loss = 0.03414898
Iteration 122, loss = 0.03630250
Iteration 123, loss = 0.03226735
Iteration 124, loss = 0.03217177
Iteration 125, loss = 0.03508762
Iteration 126, loss = 0.03087194
Iteration 127, loss = 0.03070441
Iteration 128, loss = 0.03495795
Iteration 129, loss = 0.03553228
Iteration 130, loss = 0.02929708
Iteration 131, loss = 0.02887428
Iteration 132, loss = 0.02824636
Iteration 133, loss = 0.02820505
Iteration 134, loss = 0.02823490
Iteration 135, loss = 0.02812555
Iteration 136, loss = 0.02598399
Iteration 137, loss = 0.02604164
Iteration 138, loss = 0.02701313
Iteration 139, loss = 0.02526035
Iteration 140, loss = 0.02544090
Iteration 141, loss = 0.02684534
Iteration 142, loss = 0.02523357
Iteration 143, loss = 0.02563942
Iteration 144, loss = 0.02395473
Iteration 145, loss = 0.02285817
Iteration 146, loss = 0.02292797
Iteration 147, loss = 0.02277832
Iteration 148, loss = 0.02437741
Iteration 149, loss = 0.02438440
Iteration 150, loss = 0.02378861
Iteration 151, loss = 0.02398832
Iteration 152, loss = 0.02325453
Iteration 153, loss = 0.02270355
Iteration 154, loss = 0.02155794
Iteration 155, loss = 0.02186094
Iteration 156, loss = 0.02153329
Iteration 157, loss = 0.02057954
Iteration 158, loss = 0.02009189
Iteration 159, loss = 0.01950661
Iteration 160, loss = 0.02169270
Iteration 161, loss = 0.02084339
Iteration 162, loss = 0.01923916
Iteration 163, loss = 0.01976974
Iteration 164, loss = 0.01875469
Iteration 165, loss = 0.01847235
Iteration 166, loss = 0.01940303
Iteration 167, loss = 0.01956117
Iteration 168, loss = 0.01792653
Iteration 169, loss = 0.01745507
Iteration 170, loss = 0.01882498
Iteration 171, loss = 0.01890765
Iteration 172, loss = 0.01979480
Iteration 173, loss = 0.01796924
Iteration 174, loss = 0.01943356
Iteration 175, loss = 0.01743437
Iteration 176, loss = 0.01702264
Iteration 177, loss = 0.01767637
Iteration 178, loss = 0.01730829
Iteration 179, loss = 0.01633254
Iteration 180, loss = 0.01655988
Iteration 181, loss = 0.01759534
Iteration 182, loss = 0.01707861
Iteration 183, loss = 0.01699047
Iteration 184, loss = 0.01548326
Iteration 185, loss = 0.01571801
Iteration 186, loss = 0.01612208
Iteration 187, loss = 0.02436364
Iteration 188, loss = 0.01944558
Iteration 189, loss = 0.01731004
Iteration 190, loss = 0.01592359
Iteration 191, loss = 0.01471943
Iteration 192, loss = 0.01485533
Iteration 193, loss = 0.01432749
Iteration 194, loss = 0.01490810
Iteration 195, loss = 0.01529569
Iteration 196, loss = 0.01465235
Iteration 197, loss = 0.01812936
Iteration 198, loss = 0.01869618
Iteration 199, loss = 0.01590489
Iteration 200, loss = 0.01497168
Iteration 1, loss = 0.65124993
Iteration 2, loss = 0.56700186
Iteration 3, loss = 0.50152263
Iteration 4, loss = 0.45480750
Iteration 5, loss = 0.42083399
Iteration 6, loss = 0.39511979
Iteration 7, loss = 0.37510633
Iteration 8, loss = 0.36735415
Iteration 9, loss = 0.34706075
Iteration 10, loss = 0.33696218
Iteration 11, loss = 0.32710416
Iteration 12, loss = 0.32026893
Iteration 13, loss = 0.31282819
Iteration 14, loss = 0.30476693
Iteration 15, loss = 0.29752569
Iteration 16, loss = 0.28980958
Iteration 17, loss = 0.28486484
Iteration 18, loss = 0.27938069
Iteration 19, loss = 0.27384935
Iteration 20, loss = 0.26872797
Iteration 21, loss = 0.26205083
Iteration 22, loss = 0.25746440
Iteration 23, loss = 0.24972758
Iteration 24, loss = 0.24758657
Iteration 25, loss = 0.24008350
Iteration 26, loss = 0.23656352
Iteration 27, loss = 0.23361803
Iteration 28, loss = 0.23093954
Iteration 29, loss = 0.22240988
Iteration 30, loss = 0.22316615
Iteration 31, loss = 0.21691606
Iteration 32, loss = 0.20972866
Iteration 33, loss = 0.20593559
Iteration 34, loss = 0.20087466
Iteration 35, loss = 0.19906527
Iteration 36, loss = 0.19243863
Iteration 37, loss = 0.18763863
Iteration 38, loss = 0.18296301
Iteration 39, loss = 0.18016055
Iteration 40, loss = 0.17682488
Iteration 41, loss = 0.17327583
Iteration 42, loss = 0.16930322
Iteration 43, loss = 0.16840991
Iteration 44, loss = 0.16018471
Iteration 45, loss = 0.15873177
Iteration 46, loss = 0.15386202
Iteration 47, loss = 0.15372376
Iteration 48, loss = 0.15075885
Iteration 49, loss = 0.15081652
Iteration 50, loss = 0.14199607
Iteration 51, loss = 0.13799967
Iteration 52, loss = 0.13500112
Iteration 53, loss = 0.13123984
Iteration 54, loss = 0.13615557
Iteration 55, loss = 0.12919054
Iteration 56, loss = 0.12312754
Iteration 57, loss = 0.12027173
Iteration 58, loss = 0.11621776
Iteration 59, loss = 0.11927156
Iteration 60, loss = 0.11792713
Iteration 61, loss = 0.11242977
Iteration 62, loss = 0.10685628
Iteration 63, loss = 0.10579030
Iteration 64, loss = 0.10556178
Iteration 65, loss = 0.10104623
Iteration 66, loss = 0.09896096
Iteration 67, loss = 0.09692782
Iteration 68, loss = 0.09736140
Iteration 69, loss = 0.09485986
Iteration 70, loss = 0.08966848
Iteration 71, loss = 0.08835675
Iteration 72, loss = 0.08564569
Iteration 73, loss = 0.08496730
Iteration 74, loss = 0.08524558
Iteration 75, loss = 0.08085617
Iteration 76, loss = 0.08034619
Iteration 77, loss = 0.07836747
Iteration 78, loss = 0.07707411
Iteration 79, loss = 0.07399563
Iteration 80, loss = 0.07266710
Iteration 81, loss = 0.07383185
Iteration 82, loss = 0.07118275
Iteration 83, loss = 0.07107070
Iteration 84, loss = 0.06599968
Iteration 85, loss = 0.06478628
Iteration 86, loss = 0.06398177
Iteration 87, loss = 0.06253611
Iteration 88, loss = 0.06086042
Iteration 89, loss = 0.06110681
Iteration 90, loss = 0.05835349
Iteration 91, loss = 0.05750368
Iteration 92, loss = 0.05839655
Iteration 93, loss = 0.05692087
Iteration 94, loss = 0.05403121
Iteration 95, loss = 0.05327802
Iteration 96, loss = 0.05293196
Iteration 97, loss = 0.05167784
Iteration 98, loss = 0.05173441
Iteration 99, loss = 0.05019663
Iteration 100, loss = 0.04949364
Iteration 101, loss = 0.04787691
Iteration 102, loss = 0.04632792
Iteration 103, loss = 0.04696056
Iteration 104, loss = 0.04598601
Iteration 105, loss = 0.04386783
Iteration 106, loss = 0.04365809
Iteration 107, loss = 0.04411979
Iteration 108, loss = 0.04341784
Iteration 109, loss = 0.04091502
Iteration 110, loss = 0.04037498
Iteration 111, loss = 0.04071958
Iteration 112, loss = 0.03933173
Iteration 113, loss = 0.03869065
Iteration 114, loss = 0.03732738
Iteration 115, loss = 0.03723208
Iteration 116, loss = 0.03745886
Iteration 117, loss = 0.03652293
Iteration 118, loss = 0.03613348
Iteration 119, loss = 0.03472513
Iteration 120, loss = 0.03408184
Iteration 121, loss = 0.03359841
Iteration 122, loss = 0.03783682
Iteration 123, loss = 0.03588648
Iteration 124, loss = 0.03804345
Iteration 125, loss = 0.03372079
Iteration 126, loss = 0.03291060
Iteration 127, loss = 0.03112924
Iteration 128, loss = 0.03037371
Iteration 129, loss = 0.03142739
Iteration 130, loss = 0.03112120
Iteration 131, loss = 0.03173130
Iteration 132, loss = 0.02999977
Iteration 133, loss = 0.02919709
Iteration 134, loss = 0.02913860
Iteration 135, loss = 0.03328551
Iteration 136, loss = 0.03095626
Iteration 137, loss = 0.02808762
Iteration 138, loss = 0.02678336
Iteration 139, loss = 0.02720911
Iteration 140, loss = 0.02729436
Iteration 141, loss = 0.02769023
Iteration 142, loss = 0.02607995
Iteration 143, loss = 0.02521603
Iteration 144, loss = 0.02666374
Iteration 145, loss = 0.02596695
Iteration 146, loss = 0.02615093
Iteration 147, loss = 0.02497433
Iteration 148, loss = 0.02454441
Iteration 149, loss = 0.02555289
Iteration 150, loss = 0.02535325
Iteration 151, loss = 0.02592821
Iteration 152, loss = 0.02449131
Iteration 153, loss = 0.02320298
Iteration 154, loss = 0.02519389
Iteration 155, loss = 0.02403576
Iteration 156, loss = 0.02356489
Iteration 157, loss = 0.02462675
Iteration 158, loss = 0.02355882
Iteration 159, loss = 0.02325873
Iteration 160, loss = 0.02173923
Iteration 161, loss = 0.02261933
Iteration 162, loss = 0.02157112
Iteration 163, loss = 0.02120455
Iteration 164, loss = 0.02126043
Iteration 165, loss = 0.02114595
Iteration 166, loss = 0.02077778
Iteration 167, loss = 0.02158430
Iteration 168, loss = 0.02234269
Iteration 169, loss = 0.02059933
Iteration 170, loss = 0.02063500
Iteration 171, loss = 0.02909535
Iteration 172, loss = 0.02942771
Iteration 173, loss = 0.02542841
Iteration 174, loss = 0.02156787
Iteration 175, loss = 0.02048572
Iteration 176, loss = 0.02200156
Iteration 177, loss = 0.02259745
Iteration 178, loss = 0.02022105
Iteration 179, loss = 0.02204497
Iteration 180, loss = 0.02135001
Iteration 181, loss = 0.02073023
Iteration 182, loss = 0.02017470
Iteration 183, loss = 0.01843009
Iteration 184, loss = 0.01853723
Iteration 185, loss = 0.01907858
Iteration 186, loss = 0.01904856
Iteration 187, loss = 0.01843788
Iteration 188, loss = 0.01836726
Iteration 189, loss = 0.01760469
Iteration 190, loss = 0.02320560
Iteration 191, loss = 0.02126911
Iteration 192, loss = 0.01860097
Iteration 193, loss = 0.01969215
Iteration 194, loss = 0.01863486
Iteration 195, loss = 0.02740824
Iteration 196, loss = 0.02164409
Iteration 197, loss = 0.02239041
Iteration 198, loss = 0.02047340
Iteration 199, loss = 0.01849365
Iteration 200, loss = 0.02124471
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Iteration 1, loss = 0.67263122
Iteration 2, loss = 0.61579930
Iteration 3, loss = 0.57174208
Iteration 4, loss = 0.53537043
Iteration 5, loss = 0.50368267
Iteration 6, loss = 0.48238159
Iteration 7, loss = 0.46299545
Iteration 8, loss = 0.44691401
Iteration 9, loss = 0.43210941
Iteration 10, loss = 0.42084087
Iteration 11, loss = 0.41362645
Iteration 12, loss = 0.40097279
Iteration 13, loss = 0.38887628
Iteration 14, loss = 0.38208650
Iteration 15, loss = 0.37206144
Iteration 16, loss = 0.36479284
Iteration 17, loss = 0.35825947
Iteration 18, loss = 0.34871146
Iteration 19, loss = 0.34130890
Iteration 20, loss = 0.33593588
Iteration 21, loss = 0.33375718
Iteration 22, loss = 0.32511992
Iteration 23, loss = 0.31711437
Iteration 24, loss = 0.31158195
Iteration 25, loss = 0.30357625
Iteration 26, loss = 0.29816898
Iteration 27, loss = 0.28969618
Iteration 28, loss = 0.28775933
Iteration 29, loss = 0.28056699
Iteration 30, loss = 0.27479384
Iteration 31, loss = 0.27000807
Iteration 32, loss = 0.26335997
Iteration 33, loss = 0.25859549
Iteration 34, loss = 0.25423623
Iteration 35, loss = 0.24967806
Iteration 36, loss = 0.24483565
Iteration 37, loss = 0.23824171
Iteration 38, loss = 0.23711820
Iteration 39, loss = 0.23269801
Iteration 40, loss = 0.24212247
Iteration 41, loss = 0.22262530
Iteration 42, loss = 0.21666136
Iteration 43, loss = 0.21746165
Iteration 44, loss = 0.20897930
Iteration 45, loss = 0.20492504
Iteration 46, loss = 0.20111735
Iteration 47, loss = 0.19567393
Iteration 48, loss = 0.19430959
Iteration 49, loss = 0.18702797
Iteration 50, loss = 0.18804501
Iteration 51, loss = 0.18320622
Iteration 52, loss = 0.18135593
Iteration 53, loss = 0.17484707
Iteration 54, loss = 0.17106041
Iteration 55, loss = 0.16682159
Iteration 56, loss = 0.16446410
Iteration 57, loss = 0.16022512
Iteration 58, loss = 0.15738495
Iteration 59, loss = 0.15435448
Iteration 60, loss = 0.15566862
Iteration 61, loss = 0.14929464
Iteration 62, loss = 0.15276504
Iteration 63, loss = 0.14207212
Iteration 64, loss = 0.13727110
Iteration 65, loss = 0.13604776
Iteration 66, loss = 0.13427819
Iteration 67, loss = 0.13067399
Iteration 68, loss = 0.12980100
Iteration 69, loss = 0.12849026
Iteration 70, loss = 0.12951230
Iteration 71, loss = 0.12175622
Iteration 72, loss = 0.11850173
Iteration 73, loss = 0.11727157
Iteration 74, loss = 0.11273150
Iteration 75, loss = 0.11105114
Iteration 76, loss = 0.10944477
Iteration 77, loss = 0.10664260
Iteration 78, loss = 0.10457127
Iteration 79, loss = 0.10252920
Iteration 80, loss = 0.09922530
Iteration 81, loss = 0.09671112
Iteration 82, loss = 0.09496275
Iteration 83, loss = 0.09378071
Iteration 84, loss = 0.09055362
Iteration 85, loss = 0.09044277
Iteration 86, loss = 0.08809810
Iteration 87, loss = 0.08447184
Iteration 88, loss = 0.08280238
Iteration 89, loss = 0.08324599
Iteration 90, loss = 0.08204584
Iteration 91, loss = 0.07890545
Iteration 92, loss = 0.07915275
Iteration 93, loss = 0.07630058
Iteration 94, loss = 0.07388133
Iteration 95, loss = 0.07612838
Iteration 96, loss = 0.07322782
Iteration 97, loss = 0.07066493
Iteration 98, loss = 0.06906256
Iteration 99, loss = 0.06728451
Iteration 100, loss = 0.06649361
Iteration 101, loss = 0.06537357
Iteration 102, loss = 0.06303863
Iteration 103, loss = 0.06232145
Iteration 104, loss = 0.06203468
Iteration 105, loss = 0.06238856
Iteration 106, loss = 0.06045508
Iteration 107, loss = 0.05748885
Iteration 108, loss = 0.05659276
Iteration 109, loss = 0.05709375
Iteration 110, loss = 0.05699322
Iteration 111, loss = 0.05295617
Iteration 112, loss = 0.05244747
Iteration 113, loss = 0.05303288
Iteration 114, loss = 0.04993085
Iteration 115, loss = 0.05019290
Iteration 116, loss = 0.05110001
Iteration 117, loss = 0.04817858
Iteration 118, loss = 0.04886121
Iteration 119, loss = 0.04615860
Iteration 120, loss = 0.04477805
Iteration 121, loss = 0.04899069
Iteration 122, loss = 0.04628480
Iteration 123, loss = 0.04525065
Iteration 124, loss = 0.04288885
Iteration 125, loss = 0.04233038
Iteration 126, loss = 0.04056980
Iteration 127, loss = 0.04028212
Iteration 128, loss = 0.03979441
Iteration 129, loss = 0.03918523
Iteration 130, loss = 0.04093101
Iteration 131, loss = 0.03735056
Iteration 132, loss = 0.03772117
Iteration 133, loss = 0.03644076
Iteration 134, loss = 0.03599506
Iteration 135, loss = 0.03616912
Iteration 136, loss = 0.03669977
Iteration 137, loss = 0.03772479
Iteration 138, loss = 0.03530877
Iteration 139, loss = 0.03707742
Iteration 140, loss = 0.03601351
Iteration 141, loss = 0.03449857
Iteration 142, loss = 0.03287373
Iteration 143, loss = 0.03192829
Iteration 144, loss = 0.03138612
Iteration 145, loss = 0.03175408
Iteration 146, loss = 0.03108129
Iteration 147, loss = 0.03017953
Iteration 148, loss = 0.02919694
Iteration 149, loss = 0.02946732
Iteration 150, loss = 0.03017512
Iteration 151, loss = 0.03049249
Iteration 152, loss = 0.03068555
Iteration 153, loss = 0.02900176
Iteration 154, loss = 0.02848814
Iteration 155, loss = 0.02906639
Iteration 156, loss = 0.02757697
Iteration 157, loss = 0.02677932
Iteration 158, loss = 0.02545322
Iteration 159, loss = 0.02697130
Iteration 160, loss = 0.02548838
Iteration 161, loss = 0.02769013
Iteration 162, loss = 0.02641628
Iteration 163, loss = 0.02489325
Iteration 164, loss = 0.02610682
Iteration 165, loss = 0.03978298
Iteration 166, loss = 0.02841700
Iteration 167, loss = 0.02396677
Iteration 168, loss = 0.02714845
Iteration 169, loss = 0.02552478
Iteration 170, loss = 0.02305077
Iteration 171, loss = 0.02236769
Iteration 172, loss = 0.02245784
Iteration 173, loss = 0.02214494
Iteration 174, loss = 0.02579783
Iteration 175, loss = 0.02371362
Iteration 176, loss = 0.02236568
Iteration 177, loss = 0.02110865
Iteration 178, loss = 0.02097484
Iteration 179, loss = 0.02044968
Iteration 180, loss = 0.02321593
Iteration 181, loss = 0.02223207
Iteration 182, loss = 0.02126538
Iteration 183, loss = 0.02062815
Iteration 184, loss = 0.01956214
Iteration 185, loss = 0.01960470
Iteration 186, loss = 0.01972448
Iteration 187, loss = 0.01916437
Iteration 188, loss = 0.02089018
Iteration 189, loss = 0.02162534
Iteration 190, loss = 0.01949741
Iteration 191, loss = 0.01925211
Iteration 192, loss = 0.02134846
Iteration 193, loss = 0.02120793
Iteration 194, loss = 0.02327508
Iteration 195, loss = 0.02626315
Iteration 196, loss = 0.02215727
Iteration 197, loss = 0.01930175
Iteration 198, loss = 0.01970071
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
